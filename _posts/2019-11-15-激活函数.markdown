---
layout: post
title: tensorflow_激活函数
date: "2019-11-14 14:00:000"
tags: [tensorflow, 激活函数]

---

## 流程:

```
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import matplotlib

matplotlib.use('TkAgg')

"""
阈值激活函数用于 McCulloch Pitts 神经元和原始的感知机。这是不可微的，在 x=0 时是不连续的。因此，使用这个激活函数来进行基于梯度下降或其变体的训练是不可能的。
Sigmoid 激活函数一度很受欢迎，从曲线来看，它像一个连续版的阈值激活函数。它受到梯度消失问题的困扰，即函数的梯度在两个边缘附近变为零。这使得训练和优化变得困难。
双曲正切激活函数在形状上也是 S 形并具有非线性特性。该函数以 0 为中心，与 Sigmoid 函数相比具有更陡峭的导数。与 Sigmoid 函数一样，它也受到梯度消失问题的影响。
线性激活函数是线性的。该函数是双边都趋于无穷的 [-inf，inf]。它的线性是主要问题。线性函数之和是线性函数，线性函数的线性函数也是线性函数。因此，使用这个函数，不能表示复杂数据集中存在的非线性。
ReLU 激活函数是线性激活功能的整流版本，这种整流功能允许其用于多层时捕获非线性。

使用 ReLU 的主要优点之一是导致稀疏激活。在任何时刻，所有神经元的负的输入值都不会激活神经元。就计算量来说，这使得网络在计算方面更轻便。

ReLU 神经元存在死亡 ReLU 的问题，也就是说，那些没有激活的神经元的梯度为零，因此将无法进行任何训练，并停留在死亡状态。尽管存在这个问题，但 ReLU 仍是隐藏层最常用的激活函数之一。
Softmax 激活函数被广泛用作输出层的激活函数，该函数的范围是 [0，1]。在多类分类问题中，它被用来表示一个类的概率。所有单位输出和总是 1。
"""
#普通阈值
def threshold(x):
    cond=tf.less(x,tf.zeros(tf.shape(x),dtype=x.dtype))
    out=tf.where(cond,tf.zeros(tf.shape(x)),tf.ones(tf.shape(x)))
    return out
#sigmid函数  范围在 0 到 1
def sigmid_threshold(x):
   out=tf.sigmoid(x)
   return out

#双曲线正切  中心位置是 0，其范围是从 -1 到 1
def tanh_threshold(x):
   out=tf.tanh(x)
   return out

#ReLU  负的输入值，神经元不会激活（输出为零），对于正的输入值，神经元的输出与输入值相同
def relu_threshold(x):
   out=tf.nn.relu(x)
   return out

#Softmax 是一个归一化的指数函数。一个神经元的输出不仅取决于其自身的输入值，还取决于该层中存在的所有其他神经元的输入的总和。
# 这样做的一个优点是使得神经元的输出小，因此梯度不会过大
def softmax_threshold(x):
   out=tf.nn.softmax(x)
   return out

h=np.linspace(-10,10,50)
out1=threshold(h)
out2=sigmid_threshold(h)
out3=tanh_threshold(h)
out4=relu_threshold(h)
out5=softmax_threshold(h)



init=tf.global_variables_initializer()

with tf.Session() as sess:
    sess.run(init)
    y=sess.run(out5)
    plt.xlabel("x")
    plt.ylabel("y")
    plt.title("threshold")
    plt.plot(h,y)
    plt.show()




```